{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuansongFeng/MadMario/blob/master/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SseXTspseRAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install gym-super-mario-bros==7.3.0 > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4hh1rNclV9",
        "colab_type": "text"
      },
      "source": [
        "# Welcome to Mad Mario! \n",
        "\n",
        "We put together this project to walk you through fundamentals of reinforcement learning. Along the project, you will implement a smart Mario that learns to complete levels on itself. To begin with, you don't need to know anything about Reinforcement Learning (RL). In case you wanna peek ahead, here is a [cheatsheet on RL basics](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N?usp=sharing) that we will refer to throughout the project. At the end of the tutorial, you will gain a solid understanding of RL fundamentals and implement a classic RL algorithm, Q-learning, on yourself. \n",
        "\n",
        "\n",
        "It's recommended that you have familiarity with Python and high school or equivalent level of math/statistics background -- that said, don't worry if memory is blurry. Just leave comments anywhere you feel confused, and we will explain the section in more details. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6IAcrfyegjL",
        "colab_type": "text"
      },
      "source": [
        "## Let's get started! \n",
        "\n",
        "First thing first, let's look at what we will build: Just like when we first try the game, Mario enters the game not knowing anything about the game. It makes random action just to understand the game better. Each failure experience adds to Mario's memory, and as failure accumulates, Mario starts to recognize the better action to take in a particular scenario. Eventually Mario learns a good strategy and completes the level. \n",
        "\n",
        "Let's put the story into pseudo code.\n",
        "\n",
        "```\n",
        "for a total of N episodes:\n",
        "  for a total of M steps in each episode:\n",
        "    Mario makes an action\n",
        "    Game gives a feedback \n",
        "    Mario remembers the action and feedback\n",
        "    after building up some experiences:\n",
        "      Mario learns from experiences   \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZzxAwU4f1-H",
        "colab_type": "text"
      },
      "source": [
        "In RL terminology: agent (Mario) interacts with environment (Game) by choosing actions, and environment responds with reward and next state. Based on the collected (state, action, reward) information, agent learns to maximize its future return by optimizing its action policy. \n",
        "\n",
        "While these terms may sound scary, in a short while they will all make sense. It'd be helpful to review the [cheatsheet](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N?usp=sharing), before we start coding. We begin our tutorial with the concept of Environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgK3jHa65bW9",
        "colab_type": "text"
      },
      "source": [
        "# Environment\n",
        "[Environment](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=OMfuO883blEq) is a key concept in reinforcement learning. It's the world that Mario interacts with and learns from. Environment is characterized by [state](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=36WmEZ-8bn9M). In Mario, this is the game console consisting of tubes, mushrooms and other components. When Mario makes an action, environment responds with a [reward](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=rm0EqRQqbo09) and the [next state](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=_SnLbEzua1pv).  \n",
        "\n",
        "\n",
        "Code for running the Mario environment:\n",
        "\n",
        "```\n",
        "# Initialize Super Mario environment\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "# Limit Mario action to be 1. walk right or 2. jump right\n",
        "env = JoypadSpace(\n",
        "    env,\n",
        "    [['right'],\n",
        "    ['right', 'A']]\n",
        ")\n",
        "# Start environment\n",
        "env.reset()\n",
        "for _ in range(1000):\n",
        "  # Render game output\n",
        "  env.render()\n",
        "  # Choose random action\n",
        "  action = env.action_space.sample()\n",
        "  # Perform action\n",
        "  env.step(action=action)\n",
        "# Close environment\n",
        "env.close()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bN0TFDnCpil",
        "colab_type": "text"
      },
      "source": [
        "## Wrappers\n",
        "\n",
        "A lot of times we want to perform some pre-processing to the environment before we feed its data to the agent. This introduces the idea of a wrapper.\n",
        "\n",
        "A common wrapper is one that transforms RGB images to grayscale. This reduces the size of state representation without losing much information. For the agent, its behavior doesn't change whether it lives in a RGB world or grayscale world!\n",
        "\n",
        "**before wrapper**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1c9-tUWFyk4u_vNNrkZo1Rg0e2FUcbF3N)\n",
        "\n",
        "**after wrapper**\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1ED9brgnbPmUZL43Bl_x2FDmXd-hsHBQt)\n",
        "\n",
        "\n",
        "We apply a wrapper to environment in this fashion: \n",
        "```\n",
        "env = wrapper(env, **args)\n",
        "```\n",
        "\n",
        "\n",
        "### Instructions\n",
        "\n",
        "We want to apply 3 built-in wrappers to the given `env`, `GrayScaleObservation`, `ResizeObservation`, and `FrameStack`.  \n",
        "\n",
        "https://github.com/openai/gym/tree/master/gym/wrappers\n",
        "\n",
        "\n",
        "`FrameStack` is a wrapper that will allow us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can differentiate between when Mario was landing or jumping based on his direction of movement in the previous several frames. \n",
        "\n",
        "Let's use the following arguments: \n",
        "`GrayScaleObservation`: keep_dim=False \n",
        "`ResizeObservation`: shape=84 \n",
        "`FrameStack`: num_stack=4 \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ib7vjUD5cGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import FrameStack, GrayScaleObservation\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym.spaces import Box\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    \"\"\"Downsample the image observation to a square image. \"\"\"\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
        "        return observation\n",
        "\n",
        "# the original environment object \n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "env = JoypadSpace(\n",
        "    env,\n",
        "    [['right'],\n",
        "    ['right', 'A']]\n",
        ")\n",
        "\n",
        "# TODO wrap the given env with GrayScaleObservation\n",
        "env = GrayScaleObservation(env, keep_dim=False)\n",
        "# TODO wrap the given env with ResizeObservation\n",
        "env = ResizeObservation(env, shape=84)\n",
        "# TODO wrap the given env with FrameStack \n",
        "env = FrameStack(env, num_stack=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T5DA19ABgiY",
        "colab_type": "text"
      },
      "source": [
        "## Custom Wrapper\n",
        "\n",
        "We also would like you to get a taste of implementing an environment wrapper on your own, instead of calling off-the-shelf packages. \n",
        "\n",
        "Here is an idea: to speed up training, we can skip some frames and only show every n-th frame. While some frames are skipped, it's important to accumulate all the rewards from those skipped frames. Sum all intermediate rewards and return on the n-th frame.\n",
        "\n",
        "\n",
        "### Instruction\n",
        "\n",
        "Our custom wrapper `SkipFrame` inherits from `gym.Wrapper` and we need to implement the `step()` function.\n",
        "\n",
        "During each skipped frames inside the for loop, accumulate `reward` to `total_reward`, and break if any step gives `done=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZKyjJVHQBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # TODO accumulate reward and repeat the same action \n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, info\n",
        "       \n",
        "env = SkipFrame(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILWkirWaZmx6",
        "colab_type": "text"
      },
      "source": [
        "**Final Wrapped State**\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1zZU63qsuOKZIOwWt94z6cegOF2SMEmvD)\n",
        "\n",
        "After applying the above wrappers to the environment, the final wrapped state consists of 4 gray-scaled consecutive frames stacked together, as shown above in the image on the left. Each time mario makes an action, the environment responds with a state of this structure. The structure is represented by a 3-D array of size = (4 * 84 * 84).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VUq3afemcj-",
        "colab_type": "text"
      },
      "source": [
        "# Agent\n",
        "\n",
        "Let's now turn to the other core concept in reinforcement learning: [agent](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=OMfuO883blEq). Agent interacts with the environment by making [actions](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=chyu7AVObwWP) following its [action policy](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=_SnLbEzua1pv). Let's review the pseudo code on how agent interacts with the environment:\n",
        "\n",
        "```\n",
        "for a total of N episodes:\n",
        "for a total of M steps in each episode:\n",
        "  Mario makes an action\n",
        "  Game gives a feedback\n",
        "  Mario remembers the action and feedback\n",
        "  after building up some experiences:\n",
        "    Mario learns from experiences\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmr1axsfHf4Y",
        "colab_type": "text"
      },
      "source": [
        "We create a class, `Mario`, to represent our agent in the game. `Mario` should be able to:\n",
        "\n",
        "- Choose the action to take. `Mario` acts following its [optimal action policy](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ), based on the current environment [state](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=36WmEZ-8bn9M).\n",
        "\n",
        "- Remember experiences. The experience consists of current environment state, current agent action, reward from environment and next environment state. `Mario` later uses all these experience to update its [action policy](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=_SnLbEzua1pv).\n",
        "\n",
        "- Improve action policy over time. `Mario` updates its action policy using [Q-learning](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=bBny3BgNbcmh).\n",
        "\n",
        "In following sections, we use Mario and agent interchangeably. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FMfCSqgmv23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the observation to memory\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QTdoKmKCovs",
        "colab_type": "text"
      },
      "source": [
        "## Initialize\n",
        "Before implementing any of the above functions, let's define some key parameters.\n",
        "\n",
        "### Instruction\n",
        "\n",
        "Initialize these key parameters inside `__init__()`.\n",
        "\n",
        "```\n",
        "exploration_rate: float = 1.0\n",
        "```\n",
        "\n",
        "Random Exploration Prabability. Under [some probability](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=_SnLbEzua1pv), agent does not follow the [optimal action policy](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ), but instead chooses a random action to explore the environment. A high exploration rate is important at the early stage of learning to ensure proper exploration and not falling to local optima. The exploration rate should decrease as agent improves its policy. \n",
        "\n",
        "\n",
        "```\n",
        "exploration_rate_decay: float = 0.99999975\n",
        "```\n",
        "\n",
        "Decay rate of `exploration_rate`. Agent rigorously explores space at the early stage, but gradually reduces its exploration rate to maintain action quality. In the later stage, agent already learns a fairly good policy, so we want it to follow its policy more frequently. Decrease `exploration_rate` by the factor of `exploration_rate_decay` each time the agent acts.\n",
        "\n",
        "```\n",
        "exploration_rate_min: float = 0.1\n",
        "```\n",
        "\n",
        "Minimum `exploration_rate` that Mario can decays into. Note that this value could either be `0`, in which case Mario acts completely deterministiclly, or a very small number. \n",
        "\n",
        "```\n",
        "discount_factor: float = 0.9\n",
        "```\n",
        "\n",
        "Future reward discount factor. This is $\\gamma$ in the definition of [return](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=6lmIKuxsb6qu). It serves to make agent give higher weight on the short-term rewards over future reward.\n",
        "\n",
        "\n",
        "```\n",
        "batch_size: int = 32\n",
        "```\n",
        "\n",
        "Number of experiences used to update each time.\n",
        "\n",
        "\n",
        "```\n",
        "state_dim\n",
        "```\n",
        "\n",
        "State space dimension. In Mario, this is 4 consecutive snapshots of the enviroment stacked together, where each snapshot is a 84*84 gray-scale image. This is passed in from the environment, `self.state_dim = (4, 84, 84)`. \n",
        "\n",
        "```\n",
        "action_dim\n",
        "```\n",
        "\n",
        "Action space dimension. In Mario, this is the number of total possible actions. This is passed in from environment as well. \n",
        "\n",
        "```\n",
        "memory\n",
        "```\n",
        "\n",
        "`memory` is a queue structure filled with Mario's past experiences. Each experience consists of (state, next_state, action, reward, done). As Mario collects more experiences, old experiences are popped to make room for most recent ones. We initialize the memory queue with `maxlen=100000`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxRwyg8D6gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "\n",
        "class Mario(object):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "       # state space dimension\n",
        "      self.state_dim = state_dim\n",
        "      # action space dimension\n",
        "      self.action_dim = action_dim\n",
        "      # replay buffer\n",
        "      self.memory = deque(maxlen=100000)\n",
        "      # current step, updated everytime the agent acts\n",
        "      self.step = 0\n",
        "\n",
        "      # TODO: Please initialize other variables as described above\n",
        "      self.exploration_rate = 1.0\n",
        "      self.exploration_rate_decay = 0.99999975\n",
        "      self.exploration_rate_min = 0.1\n",
        "      self.discount_factor = 0.9\n",
        "      self.batch_size = 32\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TPviLoSpAg5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Predict $Q^*$\n",
        "\n",
        "[Optimal value action function](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=snRMrCIccEx8), $Q^*(s, a)$, is the single most important function in this project. `Mario` uses it to choose the [optimal action](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ)\n",
        "\n",
        "$$\n",
        "a^*(s) = argmax_{a}Q^*(s, a)\n",
        "$$\n",
        "\n",
        "and [update its action policy](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=bBny3BgNbcmh)\n",
        "\n",
        "$$\n",
        "Q^*(s, a) \\leftarrow Q^*(s, a)+\\alpha (r + \\gamma \\max_{a'} Q^*(s', a') -Q^*(s, a))\n",
        "$$\n",
        "\n",
        "In this section, let's implement `agent.predict()` to calculate $Q^*(s, a)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwoZWE97LJXf",
        "colab_type": "text"
      },
      "source": [
        "### $Q^*_{online}$ and $Q^*_{target}$\n",
        "\n",
        "Let's review the inputs to $Q^*(s, a)$ function. \n",
        "\n",
        "$s$ is the observed state from environment. After our wrappers, $s$ is a stack of grayscale images. $a$ is a single integer representing the action taken. To deal with image/video signal, we often use a [*convolution neural network*](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). To save you time, we have created a simple `ConvNet` for you. \n",
        "\n",
        "Instead of passing action $a$ together with state $s$ into $Q^*$ function, we pass only the state. `ConvNet` returns a list of real values representing $Q^*$ for *all actions*. Later we can choose the $Q^*$ for any specific action $a$. \n",
        "\n",
        "<!-- Let's now look at Q-learning more closely.\n",
        "\n",
        "$$\n",
        "Q^*(s, a) \\leftarrow Q^*(s, a)+\\alpha (r + \\gamma \\max_{a'} Q^*(s', a') -Q^*(s, a))\n",
        "$$\n",
        "\n",
        "$(r + \\gamma \\max_{a'} Q^*(s', a'))$ is the *TD target* (cheatsheet) and $Q^*(s, a)$ is the *TD estimate* (cheatsheet). $s$ and $a$ are the current state and action, and $s'$ and $a'$ are next state and next action.  -->\n",
        "\n",
        "In this section, we define two functions: $Q_{online}$ and $Q_{target}$. *Both* represent the optimal value action function $Q^*$. Intuitively, we use $Q_{online}$ to make action decisions, and $Q_{target}$ to improve $Q_{online}$. We will explain further in details in [later sections](https://colab.research.google.com/drive/1kptUkdESbxBC-yOfSYngynjV5Hge_-t-#scrollTo=BOALqrSC5VIf). \n",
        "\n",
        "\n",
        "\n",
        "### Instructions\n",
        "\n",
        "Use our provided `ConvNet` to define `self.online_q` and `self.target_q` separately. Intialize `ConvNet` with `input_dim=self.state_dim` and `output_dim=self.action_dim` for both $Q^*$ functions. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BuvQ1-RpPF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    '''mini cnn structure\n",
        "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "    '''\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(ConvNet, self).__init__()\n",
        "        c, h, w = input_dim\n",
        "        self.conv_1 = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv_3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense = nn.Linear(3136, 512)\n",
        "        self.output = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input: B x C x H x W\n",
        "        x = input\n",
        "        x = self.conv_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv_3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "      super().__init__(state_dim, action_dim)\n",
        "      # TODO: define online action value function\n",
        "      self.online_q = ConvNet(input_dim=self.state_dim, output_dim=self.action_dim)\n",
        "      # TODO: define target action value function\n",
        "      self.target_q = ConvNet(input_dim=self.state_dim, output_dim=self.action_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY-WQ4_MCvjB",
        "colab_type": "text"
      },
      "source": [
        "### Calling $Q^*$\n",
        "\n",
        "### Instruction \n",
        "\n",
        "Both `self.online_q` and `self.target_q` are optimal value action function $Q^*$, which take a single input $s$. \n",
        "\n",
        "Implement `Mario.predict()` to calculate the $Q^*$ of input $s$. Here, $s$ is a batch of states, i.e. \n",
        "```\n",
        "shape(state) = batch_size, 4, 84, 84\n",
        "```\n",
        "\n",
        "Return $Q^*$ for all possible actions for the entire batch of states. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc1gXVO6Cv4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):\n",
        "      def predict(self, state, model):\n",
        "        \"\"\"Given a state, predict Q values of all possible actions using specified model (either online or target)\n",
        "        Input:\n",
        "          state\n",
        "           dimension of (batch_size * state_dim)\n",
        "          model\n",
        "           either 'online' or 'target'\n",
        "        Output\n",
        "          pred_q_values (torch.tensor)\n",
        "            dimension of (batch_size * action_dim), predicted Q values for all possible actions given the state\n",
        "        \"\"\"\n",
        "        # LazyFrame -> np array -> torch tensor\n",
        "        state_float = torch.FloatTensor(np.array(state))\n",
        "        # normalize\n",
        "        state_float = state_float / 255.\n",
        "        \n",
        "        if model == 'online':\n",
        "          # TODO return the predicted Q values using self.online_q\n",
        "          pred_q_values = self.online_q(state_float)\n",
        "        elif model == 'target':\n",
        "          # TODO return the predicted Q values using self.target_q\n",
        "          pred_q_values = self.target_q(state_float)\n",
        "\n",
        "        return pred_q_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU6KQb-AohNS",
        "colab_type": "text"
      },
      "source": [
        "## Act\n",
        "\n",
        "Let's now look at how Mario should `act()` in the environment. \n",
        "\n",
        "Given a state, Mario mostly [chooses the action with the highest Q value](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ). There is an *epislon* chance that Mario acts randomly instead, which encourages environment exploration. \n",
        "\n",
        "### Instruction\n",
        "\n",
        "We will use `torch.tensor` and `numpy.array` in this section. Familiarize yourself with [basic syntax with some examples](https://colab.research.google.com/drive/1D8k6i-TIKfqEHVkzKwYMjJvZRAKe9IuH?usp=sharing).  \n",
        "\n",
        "We will now implement `Mario.act()`. Recall that we have defined $Q_{online}$ above, which we will use here to calculate Q values for all actions given *state*. We then need to select the action that results in largest Q value. We have set up the logic for epsilon-greedy policy, and leave it to you to determine the optimal and random action. \n",
        "\n",
        "Before implementing `Mario.act()`, let's first get used to basic operations on *torch.tensor*, which is the data type returned in `Mario.predict()`\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW3W8i2Dpoxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action and update value of step\n",
        "        Input\n",
        "          state(np.array) \n",
        "            A single observation of the current state, dimension is (state_dim)\n",
        "        Output\n",
        "          action\n",
        "            An integer representing which action agent will perform\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "          # TODO: choose a random action from all possible actions (self.action_dim)\n",
        "          action = np.random.randint(self.action_dim)\n",
        "        else:\n",
        "          state = np.expand_dims(state, 0)\n",
        "          # TODO: choose the best action based on self.online_q\n",
        "          action_values = self.predict(state, model='online')\n",
        "          action = torch.argmax(action_values, axis=1).item()\n",
        "          \n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "        # increment step\n",
        "        self.step += 1\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyw4YkzkqgpZ",
        "colab_type": "text"
      },
      "source": [
        "## Remember\n",
        "\n",
        "In order to improve policy, Mario need to collect and save past experiences. Each time agent performs an action, it collects an experience which includes the current state, action it performs, the next state after performing the action, the reward it collected, and whether the game is finished or not. \n",
        "\n",
        "We use the `self.memory` defined above to store past experiences, consisting of (state, next_state, action, reward, done). \n",
        "\n",
        "### Instruction\n",
        "\n",
        "Implement `Mario.remember()` to save the experience to Mario's memory. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H488eKESqg59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):\n",
        "    def remember(self, experience):\n",
        "        \"\"\"Add the experience to self.memory\n",
        "        Input\n",
        "          experience =  (state, next_state, action, reward, done) tuple\n",
        "        Output\n",
        "          None\n",
        "        \"\"\"\n",
        "        # TODO Add the experience to memory\n",
        "        self.memory.append(experience)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK3j9G7Zq5OC",
        "colab_type": "text"
      },
      "source": [
        "## Learn\n",
        "\n",
        "\n",
        "The entire learning process is based on [Q-learning algorithm](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=bBny3BgNbcmh). By learning, we mean updating our $Q^*$ function to better predict the optimal value of current state-action pair. We will use both $Q^*_{online}$ and $Q^*_{target}$ in this section. \n",
        "\n",
        "\n",
        "Some key steps to perform:\n",
        "- **Experience Sampling:** \n",
        "We will sample experiences from memory as the *training data* to update $Q^*_{online}$. \n",
        "\n",
        "\n",
        "- **Evaluating TD Estimate:**\n",
        "Calculate the [TD estimate](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=2abP5k2kcRnn) of sampled experiences, using current states and actions. We use $Q^*_{online}$ in this step to directly predict $Q^*(s, a)$.  \n",
        "\n",
        "\n",
        "- **Evaluating TD Target:**\n",
        "Calculate the [TD target](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=Q072-fLecSkb) of sampled experiences, using next states and rewards. We use both $Q^*_{online}$ and $Q^*_{target}$ to calculate $r + \\gamma \\max_{a'} Q^*_{target}(s', a')$, where the $\\max_{a'}$ part is determined by $Q^*_{online}$. \n",
        "\n",
        "\n",
        "- **Loss between TD Estimate and TD Target:**\n",
        "Calculate the mean squared loss between TD estimate and TD target. \n",
        "\n",
        "\n",
        "- **Updating $Q^*_{online}$:**\n",
        "Perform an optimization step with the above calculated loss to update $Q^*_{online}$.\n",
        "\n",
        "\n",
        "Summarizing the above in pseudo code for `Mario.learn()`:\n",
        "\n",
        "```\n",
        "if enough experiences are collected:\n",
        "  sample a batch of experiences\n",
        "  calculate the predicted Q values using Q_online\n",
        "  calculate the target Q values using Q_target and reward\n",
        "  calculate loss between prediction and target Q values\n",
        "  update Q_online based on loss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb3ip_Wgp0l-",
        "colab_type": "text"
      },
      "source": [
        "### Experience Sampling \n",
        "Mario learns by drawing past experiences from its memory. The memory is a queue data structure that stores each individual experience in the format of \n",
        "\n",
        "```\n",
        "state, next_state, action, reward, done\n",
        "```\n",
        "\n",
        "Examples of some experiences in Mario's memory:\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1D34QpsmJSwHrdzszROt405ZwNY9LkTej)  next_state: ![pic](https://drive.google.com/uc?id=13j2TzRd1SGmFru9KJImZsY9DMCdqcr_J) action: jump reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v) next_state: ![pic](https://drive.google.com/uc?id=1hmGGVO1cS7N7kdcM99-K3Y2sxrAFd0Oh) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=10MHERSI6lap79VcZfHtIzCS9qT45ksk-) next_state: ![pic](https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90) action: right reward: -10.0 done: True\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1T6CAIMzNxeZlBTUdz3sB8t_GhDFbNdUO) next_state: ![pic](https://drive.google.com/uc?id=1aZlA0EnspQdcSQcVxuVmaqPW_7jT3lfW) action: jump_right reward: 0.0 done: False\n",
        "\n",
        "\n",
        "- state: ![pic](https://drive.google.com/uc?id=1bPRnGRx2c1HJ_0y_EEOFL5GOG8sUBdIo) next_state: ![pic](https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv) action: right reward: 10.0 done: False\n",
        "\n",
        "State/next_state:\n",
        "Observation at timestep *t*/*t+1*. They are both of type `LazyFrame`. \n",
        "\n",
        "Action:\n",
        "Mario's action during state transition. \n",
        "\n",
        "Reward:\n",
        "Environment's reward during state transition. \n",
        "\n",
        "Done:\n",
        "Boolean indicating if next_state is a terminal state (end of game). Terminal state has a known Q value of 0. \n",
        "\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Sample a batch of experiences from `self.memory` of size `self.batch_size`. \n",
        "\n",
        "Return a tuple of numpy arrays, in the order of (state, next_state, action, reward, done). Each numpy array should have its first dimension equal to `self.batch_size`. \n",
        "\n",
        "To convert a `LazyFrame` to numpy array, do\n",
        "\n",
        "```\n",
        "state_np_array = np.array(state_lazy_frame)\n",
        "```\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR3QdqXmhr7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "class Mario(Mario):\n",
        "  def sample_batch(self):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      self.memory (FIFO queue)\n",
        "        a queue where each entry has five elements as below\n",
        "        state: LazyFrame of dimension (state_dim)\n",
        "        next_state: LazyFrame of dimension (state_dim)\n",
        "        action: integer, representing the action taken\n",
        "        reward: float, the reward from state to next_state with action\n",
        "        done: boolean, whether state is a terminal state\n",
        "      self.batch_size (int)\n",
        "        size of the batch to return \n",
        "\n",
        "    Output\n",
        "      state, next_state, action, reward, done (tuple)\n",
        "        a tuple of numpy arrays: state, next_state, action, reward, done\n",
        "        state: numpy array of dimension (batch_size x state_dim)\n",
        "        next_state: numpy array of dimension (batch_size x state_dim)\n",
        "        action: numpy array of dimension (batch_size)\n",
        "        reward: numpy array of dimension (batch_size)\n",
        "        done: numpy array of dimension (batch_size)\n",
        "    \"\"\"\n",
        "    # TODO convert everything into numpy array \n",
        "    batch = random.sample(self.memory, self.batch_size)\n",
        "    state, next_state, action, reward, done = map(np.array, zip(*batch))\n",
        "    return state, next_state, action, reward, done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOALqrSC5VIf",
        "colab_type": "text"
      },
      "source": [
        "### TD Estimate\n",
        "\n",
        "[*TD estimate*](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=2abP5k2kcRnn) is the estimated $Q^*(s, a)$ based on *current state-action pair $s, a$*. \n",
        "\n",
        "<!-- It represents the best estimate we have so far, and the goal is to keep updating it using TD target (link to Q learning equation) We will use $Q_{online}$ to calculate this.  -->\n",
        "\n",
        "Recall our defined `Mario.predict()` above:\n",
        "```\n",
        "q_values = self.predict(state, model='online')\n",
        "```\n",
        "\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Using our defined `Mario.predict()` above, calculate the *TD Estimate* of given `state` and `action` with `online` model. Return the results in `torch.tensor` format. \n",
        "\n",
        "Note that returned values from `Mario.predict()` are $Q^*$ for all actions. To locate $Q^*$ values for specific actions, use [tensor indexing](https://colab.research.google.com/drive/1D8k6i-TIKfqEHVkzKwYMjJvZRAKe9IuH?usp=sharing). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJx-LMQmKMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):  \n",
        "  def calculate_prediction_q(self, state, action):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the current state \n",
        "      action (np.array)\n",
        "        dimension is (batch_size), each item is an integer representing the \n",
        "        action taken for current state \n",
        "\n",
        "    Output\n",
        "      pred_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a predicted Q value of the \n",
        "        current state-action pair \n",
        "    \"\"\"\n",
        "    curr_state_q = self.predict(state, model='online')\n",
        "    # TODO select specific Q values based on input actions \n",
        "    curr_state_q = curr_state_q[np.arange(0, self.batch_size), action]\n",
        "\n",
        "    return curr_state_q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG6efQFg5WCD",
        "colab_type": "text"
      },
      "source": [
        "### TD Target\n",
        "\n",
        "[*TD target*](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=Q072-fLecSkb) is the estimated $Q^*(s, a)$ *based on next state-action pair $s', a'$ and reward $r$*. \n",
        "\n",
        "*TD target* is in the form of \n",
        "\n",
        "$$\n",
        "r + \\gamma \\max_{a'} Q^*(s', a')\n",
        "$$\n",
        "\n",
        "$r$ is the current reward, $s'$ is the next state, $a'$ is the next action. \n",
        "\n",
        "### Caveats\n",
        "\n",
        "\n",
        "**Getting best next action**\n",
        "\n",
        "Because we don't know what next action $a'$ will be, we estimate it using next state $s'$ and $Q_{online}$. Specifically,\n",
        "\n",
        "$$\n",
        "a' = argmax_a Q_{online}(s', a)\n",
        "$$\n",
        "\n",
        "That is, we apply $Q_{online}$ on the next_state $s'$, and pick the action which will yield the largest Q value, and use that action to index into $Q_{target}$ to calculate *TD target* . This is why, if you compare the function signatures of `calculate_target_q` and `calculate_pred_q`, while in `calculate_prediction_q` we have `action` and `state` as an input parameter, in `calculate_target_q`  we only have `reward` and `next_state`.\n",
        "\n",
        "\n",
        "**Terminal state**\n",
        "\n",
        "Another small caveat is the terminal state, as recorded with the variable `done`, which is 1 when Mario is dead or the game finishes. \n",
        "\n",
        "Hence, we need to make sure we don't keep adding future rewards when \"there is no future\", i.e. when the game reaches terminal state. Since `done` is a boolean, we can multiply `1.0 - done` with future reward. This way, future reward after the terminal state is not taken into account in TD target.\n",
        "\n",
        "Therefore, the complete *TD target* is in the form of \n",
        "\n",
        "$$\n",
        "r + (1.0 - done) \\gamma \\max_{a'} Q^*_{target}(s', a')\n",
        "$$\n",
        "where $a'$ is determined by \n",
        "\n",
        "$$\n",
        "a' = argmax_a Q_{online}(s', a)\n",
        "$$\n",
        "\n",
        "Let's calculate *TD Target* now. \n",
        "\n",
        "## Instruction\n",
        "\n",
        "For a batch of experiences consisting of next_states $s'$ and rewards $r$, calculate the *TD target*. Note that $a'$ is not explicitly given, so we will need to first obtain that using $Q_{online}$ and next state $s'$.\n",
        "\n",
        "Return the results in `torch.tensor` format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_yrSjmhsCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario):  \n",
        "  def calculate_target_q(self, next_state, reward):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      next_state (np.array)\n",
        "        dimension is (batch_size x state_dim), each item is an observation \n",
        "        for the next state \n",
        "      reward (np.array)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      target_q (torch.tensor)\n",
        "        dimension of (batch_size), each item is a target Q value of the current\n",
        "        state-action pair, calculated based on reward collected and \n",
        "        estimated Q value for next state\n",
        "    \"\"\"\n",
        "    next_state_q = self.predict(next_state, 'target')\n",
        "\n",
        "    online_q = self.predict(next_state, 'online')\n",
        "    # TODO select the best action at next state based on online Q function\n",
        "    action_idx = torch.argmax(online_q, axis=1)\n",
        "\n",
        "    # TODO calculate target Q values based on action_idx and reward\n",
        "    target_q = torch.tensor(reward) + (1. - done) * next_state_q[np.arange(0, self.batch_size), action_idx] * self.gamma\n",
        "    \n",
        "    return target_q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkq2h_jg5Wn_",
        "colab_type": "text"
      },
      "source": [
        "### Loss\n",
        "\n",
        "Let's now calculate the loss between TD target and TD estimate. Loss is what drives the optimization and updates $Q^*_{online}$ to better predict $Q^*$ in the future. We will calculate the mean squared loss in the form of:\n",
        "\n",
        "$MSE = \\frac{1}{n}\\sum_{i=0}^n( y_i - \\hat{y}_i)^2$\n",
        "\n",
        "PyTorch already has an implementation of this loss:\n",
        "```\n",
        "loss = nn.functional.mse_loss(pred_q, target_q)\n",
        "```\n",
        "\n",
        "## Instruction\n",
        "\n",
        "Given *TD Estimate* (`pred_q`) and *TD Target* (`target_q`) for the batch of experiences, return the Mean Squared Error. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnzaTGehsHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Mario(Mario):\n",
        "  def calculate_loss(self, pred_q, target_q):\n",
        "    \"\"\"\n",
        "    Input\n",
        "      pred_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is an observation \n",
        "        for the next state \n",
        "      target_q (torch.tensor)\n",
        "        dimension is (batch_size), each item is a float representing the \n",
        "        reward collected from (state -> next state) transition \n",
        "\n",
        "    Output\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the MSE loss of pred_q and target_q\n",
        "    \"\"\"\n",
        "    # TODO calculate mean squared error loss\n",
        "    loss = nn.functional.mse_loss(pred_q, target_q)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-vyeniHBsdQ",
        "colab_type": "text"
      },
      "source": [
        "### Update $Q^*_{online}$\n",
        "\n",
        "As the final step to complete `Mario.learn()`, we use Adam optimizer to optimize upon the above calculated `loss`. This updates the parameters inside $Q^*_{online}$ function so that TD estimate gets closer to TD target. \n",
        "\n",
        "You've coded a lot so far. We got this section covered for you.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Behu1bxODdrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super().__init__(state_dim, action_dim)\n",
        "    # optimizer updates parameters in online_q using backpropagation\n",
        "    self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n",
        "\n",
        "  def update_online_q(self, loss):\n",
        "    '''\n",
        "    Input\n",
        "      loss (torch.tensor)\n",
        "        a single value representing the Huber loss of pred_q and target_q\n",
        "      optimizer\n",
        "        optimizer updates parameter in our online_q neural network to reduce\n",
        "        the loss\n",
        "    '''\n",
        "    # update online_q\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPukUy45gmkS",
        "colab_type": "text"
      },
      "source": [
        "### Update $Q^*_{target}$\n",
        "\n",
        "We need to sync $Q^*_{target}$ with $Q^*_{online}$ every once in a while, to make sure our $Q^*_{target}$ is up-to-date. We use `self.copy_every` to control how often we do the sync-up. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlnzimDHgmwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mario(Mario): \n",
        "    def sync_target_q(self):\n",
        "      \"\"\"Update target action value (Q) function with online action value (Q) function\n",
        "      \"\"\"\n",
        "      self.target_q.load_state_dict(self.online_q.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lex52LDkfmJp",
        "colab_type": "text"
      },
      "source": [
        "### Put them Together\n",
        "With all the helper methods implemented, let's revisit our `Mario.learn()` function. \n",
        "\n",
        "### Instructions\n",
        "\n",
        "We've added some logic on checking learning criterion. For the rest, use the helper methods defined above to complete `Mario.learn()` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acxIouqZflqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__(state_dim, action_dim)\n",
        "        # number of experiences to collect before training\n",
        "        self.burnin = 1e5\n",
        "        # number of experiences between updating online q\n",
        "        self.learn_every = 3\n",
        "        # number of experiences between updating target q with online q\n",
        "        self.sync_every = 1e4\n",
        "        # number of experiences between saving the current agent\n",
        "        self.save_every = 1e5\n",
        "        self.save_dir = os.path.join(\n",
        "            \"checkpoints\",\n",
        "            f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\"\n",
        "        )\n",
        "        if not os.path.exists(self.save_dir):\n",
        "            os.makedirs(self.save_dir)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save the current agent\n",
        "        \"\"\"\n",
        "        save_path = os.path.join(self.save_dir, f\"online_q_{self.step}.chkpt\")\n",
        "        torch.save(self.online_q.state_dict(), save_path)\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update prediction action value (Q) function with a batch of experiences\n",
        "        \"\"\"\n",
        "        # sync target network\n",
        "        if self.step % self.sync_every == 0:\n",
        "            self.sync_target_q()\n",
        "        # checkpoint model\n",
        "        if self.step % self.save_every == 0:\n",
        "            self.save_model()\n",
        "        # break if burn-in\n",
        "        if self.step < self.burnin:\n",
        "            return\n",
        "        # break if no training\n",
        "        if self.step % self.learn_every != 0:\n",
        "            return\n",
        "\n",
        "        # TODO: sample a batch of experiences from self.memory\n",
        "        state, next_state, action, reward, done = self.sample_batch()\n",
        "\n",
        "        # TODO: calculate prediction Q values for the batch\n",
        "        pred_q = self.calculate_prediction_q(state, action)\n",
        "\n",
        "        # TODO: calculate target Q values for the batch\n",
        "        target_q = self.calculate_target_q(next_state, reward)\n",
        "\n",
        "        # TODO: calculate huber loss of target and prediction values\n",
        "        loss = self.calculate_loss(pred_q, target_q)\n",
        "        \n",
        "        # TODO: update target network\n",
        "        self.update_online_q(loss)\n",
        "        print('udpating')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwxsUgYe9_8B",
        "colab_type": "text"
      },
      "source": [
        "# Start Learning! \n",
        "\n",
        "With the agent and environment wrappers implemented, we are ready to put Mario in the game and start learning! We will wrap the learning process in a big `for` loop that repeats the process of acting, remembering and learning by Mario. \n",
        "\n",
        "The meat of the algorithm is in the loop, let's take a closer look: \n",
        "\n",
        "### Instruction\n",
        "\n",
        "1. At the beginning of a new episode, we need to reinitialize the `state` by calling `env.reset()`\n",
        "\n",
        "2. Then we need several variables to hold the logging information we collected in this episode:\n",
        "  - `ep_reward`: reward collected in this episode\n",
        "  - `ep_length`: total length of this episode\n",
        "\n",
        "3. Now we are inside the while loop that plays the game, and we can call `env.render()` to display the visual\n",
        "\n",
        "4. We want to act by calling `Mario.act(state)` now. Remember our action follows the [action policy](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ), which is determined by $Q^*_{online}$. \n",
        "\n",
        "5. Perform the above selected action on env by calling `env.step(action)`. Collect the environment feedback: next_state, reward, if Mario is dead (done) and info. \n",
        "\n",
        "6. Store the current experience into Mario's memory, by calling `Mario.remember(exp)`. \n",
        "\n",
        "7. Learn by drawing experiences from Mario's memory and update the action policy, by calling `Mario.learn()`. \n",
        "\n",
        "8. Update logging info. \n",
        "\n",
        "9. Update state to prepare for next step. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4boS5vr4-AJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "02197a89-524d-49b9-9a71-7ca46ce29bed"
      },
      "source": [
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n)\n",
        "episodes = 10000\n",
        "\n",
        "### for Loop that train the model num_episodes times by playing the game\n",
        "\n",
        "for e in range(episodes):\n",
        "\n",
        "    # 1. Reset env/restart the game\n",
        "    state = env.reset()\n",
        "\n",
        "    # 2. Logging\n",
        "    ep_reward = 0.0\n",
        "    ep_length = 0\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # 3. Show environment (the visual)\n",
        "\n",
        "        # 4. Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # 5. Agent performs action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # 6. Remember\n",
        "        mario.remember(experience=(state, next_state, action, reward, done))\n",
        "\n",
        "        # 7. Learn \n",
        "        mario.learn()\n",
        "\n",
        "        # 8. Logging\n",
        "        ep_reward += reward\n",
        "        ep_length += 1\n",
        "\n",
        "        # 9. Update state\n",
        "        state = next_state\n",
        "\n",
        "        # If done break loop\n",
        "        if done or info['flag_get']:\n",
        "            print(f\"episode length: {ep_length}, reward: {ep_reward}\")\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode length: 465, reward: 981.0\n",
            "episode length: 175, reward: 757.0\n",
            "episode length: 163, reward: 635.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-537e0608d63c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 5. Agent performs action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 6. Remember\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-905309ae5b26>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# TODO accumulate reward and repeat the same action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nes_py/wrappers/joypad_space.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# take the step and record the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nes_py/nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# pass the action to the emulator as an unsigned byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# get the reward for this step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o7N4mNSQDK8",
        "colab_type": "text"
      },
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DikEHvE-R_DU",
        "colab_type": "text"
      },
      "source": [
        "## Off-policy\n",
        "\n",
        "Two major categories of RL algorithms are on-policy and off-policy. The algorithm we used, Q-learning, is an example of off-policy algorithm. \n",
        "\n",
        "What this means is that the experiences that Mario learns from, do not need to be generated from the current action policy. Mario is able to learn from very distant memory that are generated with an outdated action policy. In our case, how *distant* this memory could extend to is decided by `Mario.max_memory`. \n",
        "\n",
        "On-policy algorithm, on the other hand, requires that Mario learns from fresh experiences generated with current action policy. Examples include [policy gradient method](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html). \n",
        "\n",
        "\n",
        "**Why do we want to sample data points from all past experiences rather than the most recent ones(for example, from the previous episode), which are newly trained with higher accuracy?**\n",
        "\n",
        "The intuition is behind the tradeoff between these two approaches:\n",
        "\n",
        "Do we want to train on data that are generated from a small-size dataset with relatively high quality or a huge-size dataset with relatively lower quality? \n",
        "\n",
        "The answer is the latter, because the more data we have, the more of a wholistic, comprehensive point of view we have on the overall behavior of the system we have, in our case, the Mario game. Limited size dataset has the danger of overfitting and overlooking bigger pictures of the entire action/state space. \n",
        "\n",
        "\n",
        "Remember, Reinforcement Learning is all about exploring different scenarios(state) and keeping improving based on trial and errors, generated from the interactions between the **agent**(action) and the **environmental feedback**(reward). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q98Cvv5xO99V",
        "colab_type": "text"
      },
      "source": [
        "## Why two $Q^*$ functions?\n",
        "\n",
        "We defined two $Q^*$ functions, $Q^*_{online}$ and $Q^*_{target}$. Both represent the exact same thing: [optimal value action function](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=snRMrCIccEx8). We use $Q^*_{online}$ in the [TD estimate](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=2abP5k2kcRnn) and $Q^*_{target}$ in the [TD target](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=Q072-fLecSkb). This is to prevent the optimization divergence during Q-learning update:\n",
        "\n",
        "$$\n",
        "Q^*(s, a) \\leftarrow Q^*(s, a)+\\alpha (r + \\gamma \\max_{a'} Q^*(s', a') -Q^*(s, a))\n",
        "$$\n",
        "\n",
        "where 1st, 2nd and 4th $Q^*(s, a)$ are using $Q^*_{online}$, and 3rd $Q^*(s', a')$ is using $Q^*_{target}$. \n",
        "\n",
        "\n"
      ]
    }
  ]
}