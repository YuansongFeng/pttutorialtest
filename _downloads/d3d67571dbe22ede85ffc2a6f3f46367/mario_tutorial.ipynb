{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nWelcome to Mad Mario!\n=====================\n\nWe put together this project to walk you through fundamentals of\nreinforcement learning. Along the project, you will implement a smart\nMario that learns to complete levels on itself. To begin with, you don\u2019t\nneed to know anything about Reinforcement Learning (RL). In case you\nwanna peek ahead, here is a `cheatsheet on RL\nbasics <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N?usp=sharing>`__\nthat we will refer to throughout the project. At the end of the\ntutorial, you will gain a solid understanding of RL fundamentals and\nimplement a classic RL algorithm, Q-learning, on yourself.\n\nIt\u2019s recommended that you have familiarity with Python and high school\nor equivalent level of math/statistics background \u2013 that said, don\u2019t\nworry if memory is blurry. Just leave comments anywhere you feel\nconfused, and we will explain the section in more details.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# !pip install gym-super-mario-bros==7.3.0 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s get started!\n------------------\n\nFirst thing first, let\u2019s look at what we will build: Just like when we\nfirst try the game, Mario enters the game not knowing anything about the\ngame. It makes random action just to understand the game better. Each\nfailure experience adds to Mario\u2019s memory, and as failure accumulates,\nMario starts to recognize the better action to take in a particular\nscenario. Eventually Mario learns a good strategy and completes the\nlevel.\n\nLet\u2019s put the story into pseudo code.\n\n::\n\n   for a total of N episodes:\n     for a total of M steps in each episode:\n       Mario makes an action\n       Game gives a feedback\n       Mario remembers the action and feedback\n       after building up some experiences:\n         Mario learns from experiences\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In RL terminology: agent (Mario) interacts with environment (Game) by\nchoosing actions, and environment responds with reward and next state.\nBased on the collected (state, action, reward) information, agent learns\nto maximize its future return by optimizing its action policy.\n\nWhile these terms may sound scary, in a short while they will all make\nsense. It\u2019d be helpful to review the\n`cheatsheet <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N?usp=sharing>`__,\nbefore we start coding. We begin our tutorial with the concept of\nEnvironment.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Environment\n===========\n\n`Environment <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=OMfuO883blEq>`__\nis a key concept in reinforcement learning. It\u2019s the world that Mario\ninteracts with and learns from. Environment is characterized by\n`state <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=36WmEZ-8bn9M>`__.\nIn Mario, this is the game console consisting of tubes, mushrooms and\nother components. When Mario makes an action, environment responds with\na\n`reward <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=rm0EqRQqbo09>`__\nand the `next\nstate <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=_SnLbEzua1pv>`__.\n\nCode for running the Mario environment:\n\n::\n\n   # Initialize Super Mario environment\n   env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n   # Limit Mario action to be 1. walk right or 2. jump right\n   env = JoypadSpace(\n       env,\n       [['right'],\n       ['right', 'A']]\n   )\n   # Start environment\n   env.reset()\n   for _ in range(1000):\n     # Render game output\n     env.render()\n     # Choose random action\n     action = env.action_space.sample()\n     # Perform action\n     env.step(action=action)\n   # Close environment\n   env.close()\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wrappers\n--------\n\nA lot of times we want to perform some pre-processing to the environment\nbefore we feed its data to the agent. This introduces the idea of a\nwrapper.\n\nA common wrapper is one that transforms RGB images to grayscale. This\nreduces the size of state representation without losing much\ninformation. For the agent, its behavior doesn\u2019t change whether it lives\nin a RGB world or grayscale world!\n\n**before wrapper**\n\n.. figure:: https://drive.google.com/uc?id=1c9-tUWFyk4u_vNNrkZo1Rg0e2FUcbF3N\n   :alt: picture\n\n   picture\n\n**after wrapper**\n\n.. figure:: https://drive.google.com/uc?id=1ED9brgnbPmUZL43Bl_x2FDmXd-hsHBQt\n   :alt: picture\n\n   picture\n\nWe apply a wrapper to environment in this fashion:\n\n::\n\n   env = wrapper(env, **args)\n\nInstructions\n~~~~~~~~~~~~\n\nWe want to apply 3 built-in wrappers to the given ``env``,\n``GrayScaleObservation``, ``ResizeObservation``, and ``FrameStack``.\n\nhttps://github.com/openai/gym/tree/master/gym/wrappers\n\n``FrameStack`` is a wrapper that will allow us to squash consecutive\nframes of the environment into a single observation point to feed to our\nlearning model. This way, we can differentiate between when Mario was\nlanding or jumping based on his direction of movement in the previous\nseveral frames.\n\nLet\u2019s use the following arguments: ``GrayScaleObservation``:\nkeep_dim=False ``ResizeObservation``: shape=84 ``FrameStack``:\nnum_stack=4\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gym\nfrom gym.wrappers import FrameStack, GrayScaleObservation\nfrom nes_py.wrappers import JoypadSpace\nimport gym_super_mario_bros\nfrom gym.spaces import Box\nimport cv2\nimport numpy as np\n\nclass ResizeObservation(gym.ObservationWrapper):\n    \"\"\"Downsample the image observation to a square image. \"\"\"\n    def __init__(self, env, shape):\n        super().__init__(env)\n        if isinstance(shape, int):\n            self.shape = (shape, shape)\n        else:\n            self.shape = tuple(shape)\n\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def observation(self, observation):\n        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n        return observation\n\n# the original environment object\nenv = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\nenv = JoypadSpace(\n    env,\n    [['right'],\n    ['right', 'A']]\n)\n\n# TODO wrap the given env with GrayScaleObservation\nenv = GrayScaleObservation(env, keep_dim=False)\n# TODO wrap the given env with ResizeObservation\nenv = ResizeObservation(env, shape=84)\n# TODO wrap the given env with FrameStack\nenv = FrameStack(env, num_stack=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Wrapper\n--------------\n\nWe also would like you to get a taste of implementing an environment\nwrapper on your own, instead of calling off-the-shelf packages.\n\nHere is an idea: to speed up training, we can skip some frames and only\nshow every n-th frame. While some frames are skipped, it\u2019s important to\naccumulate all the rewards from those skipped frames. Sum all\nintermediate rewards and return on the n-th frame.\n\nInstruction\n~~~~~~~~~~~\n\nOur custom wrapper ``SkipFrame`` inherits from ``gym.Wrapper`` and we\nneed to implement the ``step()`` function.\n\nDuring each skipped frames inside the for loop, accumulate ``reward`` to\n``total_reward``, and break if any step gives ``done=True``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        super().__init__(env)\n        self._skip = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, and sum reward\"\"\"\n        total_reward = 0.0\n        done = False\n        for i in range(self._skip):\n            # TODO accumulate reward and repeat the same action\n            obs, reward, done, info = self.env.step(action)\n            total_reward += reward\n            if done:\n                break\n\n        return obs, total_reward, done, info\n\nenv = SkipFrame(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Final Wrapped State**\n\n.. figure:: https://drive.google.com/uc?id=1zZU63qsuOKZIOwWt94z6cegOF2SMEmvD\n   :alt: picture\n\n   picture\n\nAfter applying the above wrappers to the environment, the final wrapped\nstate consists of 4 gray-scaled consecutive frames stacked together, as\nshown above in the image on the left. Each time mario makes an action,\nthe environment responds with a state of this structure. The structure\nis represented by a 3-D array of size = (4 \\* 84 \\* 84).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agent\n=====\n\nLet\u2019s now turn to the other core concept in reinforcement learning:\n`agent <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=OMfuO883blEq>`__.\nAgent interacts with the environment by making\n`actions <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=chyu7AVObwWP>`__\nfollowing its `action\npolicy <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=_SnLbEzua1pv>`__.\nLet\u2019s review the pseudo code on how agent interacts with the\nenvironment:\n\n::\n\n   for a total of N episodes:\n   for a total of M steps in each episode:\n     Mario makes an action\n     Game gives a feedback\n     Mario remembers the action and feedback\n     after building up some experiences:\n       Mario learns from experiences\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a class, ``Mario``, to represent our agent in the game.\n``Mario`` should be able to:\n\n-  Choose the action to take. ``Mario`` acts following its `optimal\n   action\n   policy <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ>`__,\n   based on the current environment\n   `state <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=36WmEZ-8bn9M>`__.\n\n-  Remember experiences. The experience consists of current environment\n   state, current agent action, reward from environment and next\n   environment state. ``Mario`` later uses all these experience to\n   update its `action\n   policy <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=_SnLbEzua1pv>`__.\n\n-  Improve action policy over time. ``Mario`` updates its action policy\n   using\n   `Q-learning <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=bBny3BgNbcmh>`__.\n\nIn following sections, we use Mario and agent interchangeably.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario:\n    def __init__(self, state_dim, action_dim):\n        pass\n\n    def act(self, state):\n        \"\"\"Given a state, choose an epsilon-greedy action\n        \"\"\"\n        pass\n\n    def remember(self, experience):\n        \"\"\"Add the observation to memory\n        \"\"\"\n        pass\n\n    def learn(self):\n        \"\"\"Update online action value (Q) function with a batch of experiences\n        \"\"\"\n        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize\n----------\n\nBefore implementing any of the above functions, let\u2019s define some key\nparameters.\n\nInstruction\n~~~~~~~~~~~\n\nInitialize these key parameters inside ``__init__()``.\n\n::\n\n   exploration_rate: float = 1.0\n\nRandom Exploration Prabability. Under `some\nprobability <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=_SnLbEzua1pv>`__,\nagent does not follow the `optimal action\npolicy <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ>`__,\nbut instead chooses a random action to explore the environment. A high\nexploration rate is important at the early stage of learning to ensure\nproper exploration and not falling to local optima. The exploration rate\nshould decrease as agent improves its policy.\n\n::\n\n   exploration_rate_decay: float = 0.99999975\n\nDecay rate of ``exploration_rate``. Agent rigorously explores space at\nthe early stage, but gradually reduces its exploration rate to maintain\naction quality. In the later stage, agent already learns a fairly good\npolicy, so we want it to follow its policy more frequently. Decrease\n``exploration_rate`` by the factor of ``exploration_rate_decay`` each\ntime the agent acts.\n\n::\n\n   exploration_rate_min: float = 0.1\n\nMinimum ``exploration_rate`` that Mario can decays into. Note that this\nvalue could either be ``0``, in which case Mario acts completely\ndeterministiclly, or a very small number.\n\n::\n\n   discount_factor: float = 0.9\n\nFuture reward discount factor. This is $\\gamma$ in the definition\nof\n`return <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=6lmIKuxsb6qu>`__.\nIt serves to make agent give higher weight on the short-term rewards\nover future reward.\n\n::\n\n   batch_size: int = 32\n\nNumber of experiences used to update each time.\n\n::\n\n   state_dim\n\nState space dimension. In Mario, this is 4 consecutive snapshots of the\nenviroment stacked together, where each snapshot is a 84*84 gray-scale\nimage. This is passed in from the environment,\n``self.state_dim = (4, 84, 84)``.\n\n::\n\n   action_dim\n\nAction space dimension. In Mario, this is the number of total possible\nactions. This is passed in from environment as well.\n\n::\n\n   memory\n\n``memory`` is a queue structure filled with Mario\u2019s past experiences.\nEach experience consists of (state, next_state, action, reward, done).\nAs Mario collects more experiences, old experiences are popped to make\nroom for most recent ones. We initialize the memory queue with\n``maxlen=100000``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import deque\n\nclass Mario(object):\n    def __init__(self, state_dim, action_dim):\n       # state space dimension\n      self.state_dim = state_dim\n      # action space dimension\n      self.action_dim = action_dim\n      # replay buffer\n      self.memory = deque(maxlen=100000)\n      # current step, updated everytime the agent acts\n      self.step = 0\n\n      # TODO: Please initialize other variables as described above\n      self.exploration_rate = 1.0\n      self.exploration_rate_decay = 0.99999975\n      self.exploration_rate_min = 0.1\n      self.discount_factor = 0.9\n      self.batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predict $Q^*$\n-------------------\n\n`Optimal value action\nfunction <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=snRMrCIccEx8>`__,\n$Q^*(s, a)$, is the single most important function in this\nproject. ``Mario`` uses it to choose the `optimal\naction <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ>`__\n\n\\begin{align}a^*(s) = argmax_{a}Q^*(s, a)\\end{align}\n\nand `update its action\npolicy <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=bBny3BgNbcmh>`__\n\n\\begin{align}Q^*(s, a) \\leftarrow Q^*(s, a)+\\alpha (r + \\gamma \\max_{a'} Q^*(s', a') -Q^*(s, a))\\end{align}\n\nIn this section, let\u2019s implement ``agent.predict()`` to calculate\n$Q^*(s, a)$.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$Q^*_{online}$ and $Q^*_{target}$\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLet\u2019s review the inputs to $Q^*(s, a)$ function.\n\n$s$ is the observed state from environment. After our wrappers,\n$s$ is a stack of grayscale images. $a$ is a single integer\nrepresenting the action taken. To deal with image/video signal, we often\nuse a `convolution neural\nnetwork <https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`__.\nTo save you time, we have created a simple ``ConvNet`` for you.\n\nInstead of passing action $a$ together with state $s$ into\n$Q^*$ function, we pass only the state. ``ConvNet`` returns a list\nof real values representing $Q^*$ for *all actions*. Later we can\nchoose the $Q^*$ for any specific action $a$.\n\n.. raw:: html\n\n   <!-- Let's now look at Q-learning more closely.\n\n   $$\n   Q^*(s, a) \\leftarrow Q^*(s, a)+\\alpha (r + \\gamma \\max_{a'} Q^*(s', a') -Q^*(s, a))\n   $$\n\n   $(r + \\gamma \\max_{a'} Q^*(s', a'))$ is the *TD target* (cheatsheet) and $Q^*(s, a)$ is the *TD estimate* (cheatsheet). $s$ and $a$ are the current state and action, and $s'$ and $a'$ are next state and next action.  -->\n\nIn this section, we define two functions: $Q_{online}$ and\n$Q_{target}$. *Both* represent the optimal value action function\n$Q^*$. Intuitively, we use $Q_{online}$ to make action\ndecisions, and $Q_{target}$ to improve $Q_{online}$. We will\nexplain further in details in `later\nsections <https://colab.research.google.com/drive/1kptUkdESbxBC-yOfSYngynjV5Hge_-t-#scrollTo=BOALqrSC5VIf>`__.\n\nInstructions\n~~~~~~~~~~~~\n\nUse our provided ``ConvNet`` to define ``self.online_q`` and\n``self.target_q`` separately. Intialize ``ConvNet`` with\n``input_dim=self.state_dim`` and ``output_dim=self.action_dim`` for both\n$Q^*$ functions.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n\nclass ConvNet(nn.Module):\n    '''mini cnn structure\n    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n    '''\n    def __init__(self, input_dim, output_dim):\n        super(ConvNet, self).__init__()\n        c, h, w = input_dim\n        self.conv_1 = nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4)\n        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n        self.conv_3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n        self.relu = nn.ReLU()\n        self.flatten = nn.Flatten()\n        self.dense = nn.Linear(3136, 512)\n        self.output = nn.Linear(512, output_dim)\n\n    def forward(self, input):\n        # input: B x C x H x W\n        x = input\n        x = self.conv_1(x)\n        x = self.relu(x)\n        x = self.conv_2(x)\n        x = self.relu(x)\n        x = self.conv_3(x)\n        x = self.relu(x)\n\n        x = self.flatten(x)\n        x = self.dense(x)\n        x = self.relu(x)\n        x = self.output(x)\n\n        return x\n\nclass Mario(Mario):\n    def __init__(self, state_dim, action_dim):\n      super().__init__(state_dim, action_dim)\n      # TODO: define online action value function\n      self.online_q = ConvNet(input_dim=self.state_dim, output_dim=self.action_dim)\n      # TODO: define target action value function\n      self.target_q = ConvNet(input_dim=self.state_dim, output_dim=self.action_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calling $Q^*$\n~~~~~~~~~~~~~~~~~~~\n\nInstruction\n~~~~~~~~~~~\n\nBoth ``self.online_q`` and ``self.target_q`` are optimal value action\nfunction $Q^*$, which take a single input $s$.\n\nImplement ``Mario.predict()`` to calculate the $Q^*$ of input\n$s$. Here, $s$ is a batch of states, i.e.\n\n::\n\n   shape(state) = batch_size, 4, 84, 84\n\nReturn $Q^*$ for all possible actions for the entire batch of\nstates.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n      def predict(self, state, model):\n        \"\"\"Given a state, predict Q values of all possible actions using specified model (either online or target)\n        Input:\n          state\n           dimension of (batch_size * state_dim)\n          model\n           either 'online' or 'target'\n        Output\n          pred_q_values (torch.tensor)\n            dimension of (batch_size * action_dim), predicted Q values for all possible actions given the state\n        \"\"\"\n        # LazyFrame -> np array -> torch tensor\n        state_float = torch.FloatTensor(np.array(state))\n        # normalize\n        state_float = state_float / 255.\n\n        if model == 'online':\n          # TODO return the predicted Q values using self.online_q\n          pred_q_values = self.online_q(state_float)\n        elif model == 'target':\n          # TODO return the predicted Q values using self.target_q\n          pred_q_values = self.target_q(state_float)\n\n        return pred_q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Act\n---\n\nLet\u2019s now look at how Mario should ``act()`` in the environment.\n\nGiven a state, Mario mostly `chooses the action with the highest Q\nvalue <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ>`__.\nThere is an *epislon* chance that Mario acts randomly instead, which\nencourages environment exploration.\n\nInstruction\n~~~~~~~~~~~\n\nWe will use ``torch.tensor`` and ``numpy.array`` in this section.\nFamiliarize yourself with `basic syntax with some\nexamples <https://colab.research.google.com/drive/1D8k6i-TIKfqEHVkzKwYMjJvZRAKe9IuH?usp=sharing>`__.\n\nWe will now implement ``Mario.act()``. Recall that we have defined\n$Q_{online}$ above, which we will use here to calculate Q values\nfor all actions given *state*. We then need to select the action that\nresults in largest Q value. We have set up the logic for epsilon-greedy\npolicy, and leave it to you to determine the optimal and random action.\n\nBefore implementing ``Mario.act()``, let\u2019s first get used to basic\noperations on *torch.tensor*, which is the data type returned in\n``Mario.predict()``\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n    def act(self, state):\n        \"\"\"Given a state, choose an epsilon-greedy action and update value of step\n        Input\n          state(np.array)\n            A single observation of the current state, dimension is (state_dim)\n        Output\n          action\n            An integer representing which action agent will perform\n        \"\"\"\n        if np.random.rand() < self.exploration_rate:\n          # TODO: choose a random action from all possible actions (self.action_dim)\n          action = np.random.randint(self.action_dim)\n        else:\n          state = np.expand_dims(state, 0)\n          # TODO: choose the best action based on self.online_q\n          action_values = self.predict(state, model='online')\n          action = torch.argmax(action_values, axis=1).item()\n\n        # decrease exploration_rate\n        self.exploration_rate *= self.exploration_rate_decay\n        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n        # increment step\n        self.step += 1\n        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember\n--------\n\nIn order to improve policy, Mario need to collect and save past\nexperiences. Each time agent performs an action, it collects an\nexperience which includes the current state, action it performs, the\nnext state after performing the action, the reward it collected, and\nwhether the game is finished or not.\n\nWe use the ``self.memory`` defined above to store past experiences,\nconsisting of (state, next_state, action, reward, done).\n\nInstruction\n~~~~~~~~~~~\n\nImplement ``Mario.remember()`` to save the experience to Mario\u2019s memory.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n    def remember(self, experience):\n        \"\"\"Add the experience to self.memory\n        Input\n          experience =  (state, next_state, action, reward, done) tuple\n        Output\n          None\n        \"\"\"\n        # TODO Add the experience to memory\n        self.memory.append(experience)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Learn\n-----\n\nThe entire learning process is based on `Q-learning\nalgorithm <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=bBny3BgNbcmh>`__.\nBy learning, we mean updating our $Q^*$ function to better predict\nthe optimal value of current state-action pair. We will use both\n$Q^*_{online}$ and $Q^*_{target}$ in this section.\n\nSome key steps to perform: - **Experience Sampling:** We will sample\nexperiences from memory as the *training data* to update\n$Q^*_{online}$.\n\n-  **Evaluating TD Estimate:** Calculate the `TD\n   estimate <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=2abP5k2kcRnn>`__\n   of sampled experiences, using current states and actions. We use\n   $Q^*_{online}$ in this step to directly predict\n   $Q^*(s, a)$.\n\n-  **Evaluating TD Target:** Calculate the `TD\n   target <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=Q072-fLecSkb>`__\n   of sampled experiences, using next states and rewards. We use both\n   $Q^*_{online}$ and $Q^*_{target}$ to calculate\n   $r + \\gamma \\max_{a'} Q^*_{target}(s', a')$, where the\n   $\\max_{a'}$ part is determined by $Q^*_{online}$.\n\n-  **Loss between TD Estimate and TD Target:** Calculate the mean\n   squared loss between TD estimate and TD target.\n\n-  **Updating $Q^*_{online}$:** Perform an optimization step with\n   the above calculated loss to update $Q^*_{online}$.\n\nSummarizing the above in pseudo code for ``Mario.learn()``:\n\n::\n\n   if enough experiences are collected:\n     sample a batch of experiences\n     calculate the predicted Q values using Q_online\n     calculate the target Q values using Q_target and reward\n     calculate loss between prediction and target Q values\n     update Q_online based on loss\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experience Sampling\n~~~~~~~~~~~~~~~~~~~\n\nMario learns by drawing past experiences from its memory. The memory is\na queue data structure that stores each individual experience in the\nformat of\n\n::\n\n   state, next_state, action, reward, done\n\nExamples of some experiences in Mario\u2019s memory:\n\n-  state: |pic| next_state: |pic| action: jump reward: 0.0 done: False\n\n-  state: |pic| next_state: |pic| action: right reward: -10.0 done: True\n\n-  state: |pic| next_state: |pic| action: right reward: -10.0 done: True\n\n-  state: |pic| next_state: |pic| action: jump_right reward: 0.0 done:\n   False\n\n-  state: |pic| next_state: |pic| action: right reward: 10.0 done: False\n\nState/next_state: Observation at timestep *t*/*t+1*. They are both of\ntype ``LazyFrame``.\n\nAction: Mario\u2019s action during state transition.\n\nReward: Environment\u2019s reward during state transition.\n\nDone: Boolean indicating if next_state is a terminal state (end of\ngame). Terminal state has a known Q value of 0.\n\nInstruction\n-----------\n\nSample a batch of experiences from ``self.memory`` of size\n``self.batch_size``.\n\nReturn a tuple of numpy arrays, in the order of (state, next_state,\naction, reward, done). Each numpy array should have its first dimension\nequal to ``self.batch_size``.\n\nTo convert a ``LazyFrame`` to numpy array, do\n\n::\n\n   state_np_array = np.array(state_lazy_frame)\n\n.. |pic| image:: https://drive.google.com/uc?id=1D34QpsmJSwHrdzszROt405ZwNY9LkTej\n.. |pic| image:: https://drive.google.com/uc?id=13j2TzRd1SGmFru9KJImZsY9DMCdqcr_J\n.. |pic| image:: https://drive.google.com/uc?id=1ByUKXf967Z6C9FBVtsn_QRnJTr9w-18v\n.. |pic| image:: https://drive.google.com/uc?id=1hmGGVO1cS7N7kdcM99-K3Y2sxrAFd0Oh\n.. |pic| image:: https://drive.google.com/uc?id=10MHERSI6lap79VcZfHtIzCS9qT45ksk-\n.. |pic| image:: https://drive.google.com/uc?id=1VFNOwQHGAf9pH_56_w0uRO4WUJTIXG90\n.. |pic| image:: https://drive.google.com/uc?id=1T6CAIMzNxeZlBTUdz3sB8t_GhDFbNdUO\n.. |pic| image:: https://drive.google.com/uc?id=1aZlA0EnspQdcSQcVxuVmaqPW_7jT3lfW\n.. |pic| image:: https://drive.google.com/uc?id=1bPRnGRx2c1HJ_0y_EEOFL5GOG8sUBdIo\n.. |pic| image:: https://drive.google.com/uc?id=1qtR4qCURBq57UCrmObM6A5-CH26NYaHv\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\n\nclass Mario(Mario):\n  def sample_batch(self):\n    \"\"\"\n    Input\n      self.memory (FIFO queue)\n        a queue where each entry has five elements as below\n        state: LazyFrame of dimension (state_dim)\n        next_state: LazyFrame of dimension (state_dim)\n        action: integer, representing the action taken\n        reward: float, the reward from state to next_state with action\n        done: boolean, whether state is a terminal state\n      self.batch_size (int)\n        size of the batch to return\n\n    Output\n      state, next_state, action, reward, done (tuple)\n        a tuple of numpy arrays: state, next_state, action, reward, done\n        state: numpy array of dimension (batch_size x state_dim)\n        next_state: numpy array of dimension (batch_size x state_dim)\n        action: numpy array of dimension (batch_size)\n        reward: numpy array of dimension (batch_size)\n        done: numpy array of dimension (batch_size)\n    \"\"\"\n    # TODO convert everything into numpy array\n    batch = random.sample(self.memory, self.batch_size)\n    state, next_state, action, reward, done = map(np.array, zip(*batch))\n    return state, next_state, action, reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TD Estimate\n~~~~~~~~~~~\n\n`TD\nestimate <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=2abP5k2kcRnn>`__\nis the estimated $Q^*(s, a)$ based on *current state-action pair\n$s, a$*.\n\n.. raw:: html\n\n   <!-- It represents the best estimate we have so far, and the goal is to keep updating it using TD target (link to Q learning equation) We will use $Q_{online}$ to calculate this.  -->\n\nRecall our defined ``Mario.predict()`` above:\n\n::\n\n   q_values = self.predict(state, model='online')\n\nInstruction\n-----------\n\nUsing our defined ``Mario.predict()`` above, calculate the *TD Estimate*\nof given ``state`` and ``action`` with ``online`` model. Return the\nresults in ``torch.tensor`` format.\n\nNote that returned values from ``Mario.predict()`` are $Q^*$ for\nall actions. To locate $Q^*$ values for specific actions, use\n`tensor\nindexing <https://colab.research.google.com/drive/1D8k6i-TIKfqEHVkzKwYMjJvZRAKe9IuH?usp=sharing>`__.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n  def calculate_prediction_q(self, state, action):\n    \"\"\"\n    Input\n      state (np.array)\n        dimension is (batch_size x state_dim), each item is an observation\n        for the current state\n      action (np.array)\n        dimension is (batch_size), each item is an integer representing the\n        action taken for current state\n\n    Output\n      pred_q (torch.tensor)\n        dimension of (batch_size), each item is a predicted Q value of the\n        current state-action pair\n    \"\"\"\n    curr_state_q = self.predict(state, model='online')\n    # TODO select specific Q values based on input actions\n    curr_state_q = curr_state_q[np.arange(0, self.batch_size), action]\n\n    return curr_state_q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TD Target\n~~~~~~~~~\n\n`TD\ntarget <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=Q072-fLecSkb>`__\nis the estimated $Q^*(s, a)$ *based on next state-action pair\n$s', a'$ and reward $r$*.\n\n*TD target* is in the form of\n\n\\begin{align}r + \\gamma \\max_{a'} Q^*(s', a')\\end{align}\n\n$r$ is the current reward, $s'$ is the next state,\n$a'$ is the next action.\n\nCaveats\n~~~~~~~\n\n**Getting best next action**\n\nBecause we don\u2019t know what next action $a'$ will be, we estimate\nit using next state $s'$ and $Q_{online}$. Specifically,\n\n\\begin{align}a' = argmax_a Q_{online}(s', a)\\end{align}\n\nThat is, we apply $Q_{online}$ on the next_state $s'$, and\npick the action which will yield the largest Q value, and use that\naction to index into $Q_{target}$ to calculate *TD target* . This\nis why, if you compare the function signatures of ``calculate_target_q``\nand ``calculate_pred_q``, while in ``calculate_prediction_q`` we have\n``action`` and ``state`` as an input parameter, in\n``calculate_target_q`` we only have ``reward`` and ``next_state``.\n\n**Terminal state**\n\nAnother small caveat is the terminal state, as recorded with the\nvariable ``done``, which is 1 when Mario is dead or the game finishes.\n\nHence, we need to make sure we don\u2019t keep adding future rewards when\n\u201cthere is no future\u201d, i.e.\u00a0when the game reaches terminal state. Since\n``done`` is a boolean, we can multiply ``1.0 - done`` with future\nreward. This way, future reward after the terminal state is not taken\ninto account in TD target.\n\nTherefore, the complete *TD target* is in the form of\n\n\\begin{align}r + (1.0 - done) \\gamma \\max_{a'} Q^*_{target}(s', a')\\end{align}\n\n where $a'$ is determined by\n\n\\begin{align}a' = argmax_a Q_{online}(s', a)\\end{align}\n\nLet\u2019s calculate *TD Target* now.\n\nInstruction\n-----------\n\nFor a batch of experiences consisting of next_states $s'$ and\nrewards $r$, calculate the *TD target*. Note that $a'$ is\nnot explicitly given, so we will need to first obtain that using\n$Q_{online}$ and next state $s'$.\n\nReturn the results in ``torch.tensor`` format.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n  def calculate_target_q(self, next_state, reward):\n    \"\"\"\n    Input\n      next_state (np.array)\n        dimension is (batch_size x state_dim), each item is an observation\n        for the next state\n      reward (np.array)\n        dimension is (batch_size), each item is a float representing the\n        reward collected from (state -> next state) transition\n\n    Output\n      target_q (torch.tensor)\n        dimension of (batch_size), each item is a target Q value of the current\n        state-action pair, calculated based on reward collected and\n        estimated Q value for next state\n    \"\"\"\n    next_state_q = self.predict(next_state, 'target')\n\n    online_q = self.predict(next_state, 'online')\n    # TODO select the best action at next state based on online Q function\n    action_idx = torch.argmax(online_q, axis=1)\n\n    # TODO calculate target Q values based on action_idx and reward\n    target_q = torch.tensor(reward) + (1. - done) * next_state_q[np.arange(0, self.batch_size), action_idx] * self.gamma\n\n    return target_q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loss\n~~~~\n\nLet\u2019s now calculate the loss between TD target and TD estimate. Loss is\nwhat drives the optimization and updates $Q^*_{online}$ to better\npredict $Q^*$ in the future. We will calculate the mean squared\nloss in the form of:\n\n$MSE = \\frac{1}{n}\\sum_{i=0}^n( y_i - \\hat{y}_i)^2$\n\nPyTorch already has an implementation of this loss:\n\n::\n\n   loss = nn.functional.mse_loss(pred_q, target_q)\n\nInstruction\n-----------\n\nGiven *TD Estimate* (``pred_q``) and *TD Target* (``target_q``) for the\nbatch of experiences, return the Mean Squared Error.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n\nclass Mario(Mario):\n  def calculate_loss(self, pred_q, target_q):\n    \"\"\"\n    Input\n      pred_q (torch.tensor)\n        dimension is (batch_size), each item is an observation\n        for the next state\n      target_q (torch.tensor)\n        dimension is (batch_size), each item is a float representing the\n        reward collected from (state -> next state) transition\n\n    Output\n      loss (torch.tensor)\n        a single value representing the MSE loss of pred_q and target_q\n    \"\"\"\n    # TODO calculate mean squared error loss\n    loss = nn.functional.mse_loss(pred_q, target_q)\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Update $Q^*_{online}$\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAs the final step to complete ``Mario.learn()``, we use Adam optimizer\nto optimize upon the above calculated ``loss``. This updates the\nparameters inside $Q^*_{online}$ function so that TD estimate gets\ncloser to TD target.\n\nYou\u2019ve coded a lot so far. We got this section covered for you.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nclass Mario(Mario):\n  def __init__(self, state_dim, action_dim):\n    super().__init__(state_dim, action_dim)\n    # optimizer updates parameters in online_q using backpropagation\n    self.optimizer = torch.optim.Adam(self.online_q.parameters(), lr=0.00025)\n\n  def update_online_q(self, loss):\n    '''\n    Input\n      loss (torch.tensor)\n        a single value representing the Huber loss of pred_q and target_q\n      optimizer\n        optimizer updates parameter in our online_q neural network to reduce\n        the loss\n    '''\n    # update online_q\n    self.optimizer.zero_grad()\n    loss.backward()\n    self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Update $Q^*_{target}$\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe need to sync $Q^*_{target}$ with $Q^*_{online}$ every\nonce in a while, to make sure our $Q^*_{target}$ is up-to-date. We\nuse ``self.copy_every`` to control how often we do the sync-up.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n    def sync_target_q(self):\n      \"\"\"Update target action value (Q) function with online action value (Q) function\n      \"\"\"\n      self.target_q.load_state_dict(self.online_q.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Put them Together\n~~~~~~~~~~~~~~~~~\n\nWith all the helper methods implemented, let\u2019s revisit our\n``Mario.learn()`` function.\n\nInstructions\n~~~~~~~~~~~~\n\nWe\u2019ve added some logic on checking learning criterion. For the rest, use\nthe helper methods defined above to complete ``Mario.learn()`` function.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport datetime\n\nclass Mario(Mario):\n    def __init__(self, state_dim, action_dim):\n        super().__init__(state_dim, action_dim)\n        # number of experiences to collect before training\n        self.burnin = 1e5\n        # number of experiences between updating online q\n        self.learn_every = 3\n        # number of experiences between updating target q with online q\n        self.sync_every = 1e4\n        # number of experiences between saving the current agent\n        self.save_every = 1e5\n        self.save_dir = os.path.join(\n            \"checkpoints\",\n            f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')}\"\n        )\n        if not os.path.exists(self.save_dir):\n            os.makedirs(self.save_dir)\n\n    def save_model(self):\n        \"\"\"Save the current agent\n        \"\"\"\n        save_path = os.path.join(self.save_dir, f\"online_q_{self.step}.chkpt\")\n        torch.save(self.online_q.state_dict(), save_path)\n\n\n    def learn(self):\n        \"\"\"Update prediction action value (Q) function with a batch of experiences\n        \"\"\"\n        # sync target network\n        if self.step % self.sync_every == 0:\n            self.sync_target_q()\n        # checkpoint model\n        if self.step % self.save_every == 0:\n            self.save_model()\n        # break if burn-in\n        if self.step < self.burnin:\n            return\n        # break if no training\n        if self.step % self.learn_every != 0:\n            return\n\n        # TODO: sample a batch of experiences from self.memory\n        state, next_state, action, reward, done = self.sample_batch()\n\n        # TODO: calculate prediction Q values for the batch\n        pred_q = self.calculate_prediction_q(state, action)\n\n        # TODO: calculate target Q values for the batch\n        target_q = self.calculate_target_q(next_state, reward)\n\n        # TODO: calculate huber loss of target and prediction values\n        loss = self.calculate_loss(pred_q, target_q)\n\n        # TODO: update target network\n        self.update_online_q(loss)\n        print('udpating')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start Learning!\n===============\n\nWith the agent and environment wrappers implemented, we are ready to put\nMario in the game and start learning! We will wrap the learning process\nin a big ``for`` loop that repeats the process of acting, remembering\nand learning by Mario.\n\nThe meat of the algorithm is in the loop, let\u2019s take a closer look:\n\nInstruction\n~~~~~~~~~~~\n\n1. At the beginning of a new episode, we need to reinitialize the\n   ``state`` by calling ``env.reset()``\n\n2. Then we need several variables to hold the logging information we\n   collected in this episode:\n\n-  ``ep_reward``: reward collected in this episode\n-  ``ep_length``: total length of this episode\n\n3. Now we are inside the while loop that plays the game, and we can call\n   ``env.render()`` to display the visual\n\n4. We want to act by calling ``Mario.act(state)`` now. Remember our\n   action follows the `action\n   policy <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=SZ313skqbSjQ>`__,\n   which is determined by $Q^*_{online}$.\n\n5. Perform the above selected action on env by calling\n   ``env.step(action)``. Collect the environment feedback: next_state,\n   reward, if Mario is dead (done) and info.\n\n6. Store the current experience into Mario\u2019s memory, by calling\n   ``Mario.remember(exp)``.\n\n7. Learn by drawing experiences from Mario\u2019s memory and update the\n   action policy, by calling ``Mario.learn()``.\n\n8. Update logging info.\n\n9. Update state to prepare for next step.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n)\nepisodes = 10\n\n### for Loop that train the model num_episodes times by playing the game\n\nfor e in range(episodes):\n\n    # 1. Reset env/restart the game\n    state = env.reset()\n\n    # 2. Logging\n    ep_reward = 0.0\n    ep_length = 0\n\n    # Play the game!\n    while True:\n\n        # 3. Show environment (the visual)\n\n        # 4. Run agent on the state\n        action = mario.act(state)\n\n        # 5. Agent performs action\n        next_state, reward, done, info = env.step(action)\n\n        # 6. Remember\n        mario.remember(experience=(state, next_state, action, reward, done))\n\n        # 7. Learn\n        mario.learn()\n\n        # 8. Logging\n        ep_reward += reward\n        ep_length += 1\n\n        # 9. Update state\n        state = next_state\n\n        # If done break loop\n        if done or info['flag_get']:\n            print(f\"episode length: {ep_length}, reward: {ep_reward}\")\n            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion\n==========\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Off-policy\n----------\n\nTwo major categories of RL algorithms are on-policy and off-policy. The\nalgorithm we used, Q-learning, is an example of off-policy algorithm.\n\nWhat this means is that the experiences that Mario learns from, do not\nneed to be generated from the current action policy. Mario is able to\nlearn from very distant memory that are generated with an outdated\naction policy. In our case, how *distant* this memory could extend to is\ndecided by ``Mario.max_memory``.\n\nOn-policy algorithm, on the other hand, requires that Mario learns from\nfresh experiences generated with current action policy. Examples include\n`policy gradient\nmethod <https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html>`__.\n\n**Why do we want to sample data points from all past experiences rather\nthan the most recent ones(for example, from the previous episode), which\nare newly trained with higher accuracy?**\n\nThe intuition is behind the tradeoff between these two approaches:\n\nDo we want to train on data that are generated from a small-size dataset\nwith relatively high quality or a huge-size dataset with relatively\nlower quality?\n\nThe answer is the latter, because the more data we have, the more of a\nwholistic, comprehensive point of view we have on the overall behavior\nof the system we have, in our case, the Mario game. Limited size dataset\nhas the danger of overfitting and overlooking bigger pictures of the\nentire action/state space.\n\nRemember, Reinforcement Learning is all about exploring different\nscenarios(state) and keeping improving based on trial and errors,\ngenerated from the interactions between the **agent**\\ (action) and the\n**environmental feedback**\\ (reward).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why two $Q^*$ functions?\n------------------------------\n\nWe defined two $Q^*$ functions, $Q^*_{online}$ and\n$Q^*_{target}$. Both represent the exact same thing: `optimal\nvalue action\nfunction <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=snRMrCIccEx8>`__.\nWe use $Q^*_{online}$ in the `TD\nestimate <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=2abP5k2kcRnn>`__\nand $Q^*_{target}$ in the `TD\ntarget <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N#scrollTo=Q072-fLecSkb>`__.\nThis is to prevent the optimization divergence during Q-learning update:\n\n\\begin{align}Q^*(s, a) \\leftarrow Q^*(s, a)+\\alpha (r + \\gamma \\max_{a'} Q^*(s', a') -Q^*(s, a))\\end{align}\n\nwhere 1st, 2nd and 4th $Q^*(s, a)$ are using $Q^*_{online}$,\nand 3rd $Q^*(s', a')$ is using $Q^*_{target}$.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}